{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xml.etree.ElementTree as ET\n",
    "\n",
    "# # fichier_xml = 'data/deft09_parlement_appr_fr.xml'\n",
    "# fichier_xml = 'deft09_parlement_test_fr.xml'\n",
    "# tree = ET.parse(fichier_xml)\n",
    "# root = tree.getroot()\n",
    "\n",
    "# list_contenu = []\n",
    "\n",
    "# for texte_element in root.iter('texte'):\n",
    "#     contenu = ET.tostring(texte_element, encoding='unicode', method='text').strip()\n",
    "#     list_contenu.append(contenu)\n",
    "# text_content = ' '.join(list_contenu)\n",
    "\n",
    "# print(text_content)\n",
    "\n",
    "# import xml.etree.ElementTree as ET\n",
    "# import pandas as pd\n",
    "# import random\n",
    "# import csv\n",
    "\n",
    "# tree = ET.parse(fichier_xml)\n",
    "# root = tree.getroot()\n",
    "\n",
    "# # Initialiser une liste pour stocker les données\n",
    "# data = []\n",
    "\n",
    "# # Parcourir les documents dans le fichier XML\n",
    "# for doc in root.findall('doc'):\n",
    "#     doc_id = doc.attrib['id']\n",
    "    \n",
    "#     # parti = doc.find('EVALUATION/EVAL_PARTI/PARTI').attrib['valeur']\n",
    "    \n",
    "#     texte = doc.find('texte/p').text\n",
    "\n",
    "#     # if parti in ['ELDR', 'GUE-NGL', 'PPE-DE', 'PSE', 'Verts-ALE']:  # Limiter aux principaux partis\n",
    "#     data.append([doc_id, \"Anonyme\", texte])\n",
    "\n",
    "# fichier_csv = 'test_corpus_fr.csv'\n",
    "# with open(fichier_csv, mode='w', newline='', encoding='utf-8') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerow(['ID', 'Intervenant', 'Texte', 'Parti Politique'])\n",
    "#     writer.writerows(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pandas as pd\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "\n",
    "custom_stop_words = set(stopwords.words('french'))\n",
    "custom_stop_words = set(stopwords.words('french') + [\n",
    "    'monsieur', 'madame', 'président', 'présidente', \n",
    "    'commissaire', 'groupe', 'rapport', 'parlement'\n",
    "])\n",
    "\n",
    "# Prétraitement avancé\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import spacy\n",
    "\n",
    "# Charger le modèle français de SpaCy\n",
    "# nlp = spacy.load(\"fr_core_news_md\")\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "\n",
    "# Liste de stopwords personnalisée\n",
    "custom_stop_words = set(stopwords.words('french') + [\n",
    "    'monsieur', 'madame', 'président', 'présidente', \n",
    "    'commissaire', 'groupe', 'rapport', 'parlement'\n",
    "])\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Prétraitement avancé du texte : \n",
    "    - Minuscules\n",
    "    - Suppression des ponctuations et caractères spéciaux\n",
    "    - Suppression des stopwords\n",
    "    - Lemmatisation\n",
    "    \"\"\"\n",
    "    # Convertir en minuscules\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Supprimer les balises HTML et caractères spéciaux\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)  # Supprime les balises HTML\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)  # Supprime les caractères spéciaux\n",
    "    \n",
    "    # Tokenization avec SpaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Lemmatisation, suppression des mots vides et des mots très courts\n",
    "    tokens = [\n",
    "        token.lemma_ for token in doc \n",
    "        if token.is_alpha and  # Garder uniquement les mots (pas de chiffres)\n",
    "        len(token) > 2 and     # Supprimer les mots très courts\n",
    "        token.lemma_ not in custom_stop_words\n",
    "    ]\n",
    "    \n",
    "    # Joindre les tokens pour reconstruire le texte\n",
    "    processed_text = \" \".join(tokens)\n",
    "    return processed_text\n",
    "\n",
    "\n",
    "# Chargement et prétraitement des données\n",
    "def load_and_preprocess_data(train_path, test_path):\n",
    "    # Données d'entraînement\n",
    "    train_data = pd.read_csv(train_path)\n",
    "    train_data['Texte'] = train_data['Texte'].fillna(\"\")\n",
    "    # .apply(preprocess_text)\n",
    "    \n",
    "    # Données de test\n",
    "    test_data = pd.read_csv(test_path)\n",
    "    test_data['Texte'] = test_data['Texte'].fillna(\"\")\n",
    "    # .apply(preprocess_text)\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "# Charger les données\n",
    "train_df, test_df = load_and_preprocess_data(\"corpus_fr.csv\", \"test_corpus_fr.csv\")\n",
    "\n",
    "# Préparation pour le machine learning\n",
    "X_data_train = train_df['Texte']\n",
    "y_data_train = train_df['Parti Politique']\n",
    "data_test = test_df['Texte']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open(\"data/deft09_parlement_ref_fr.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    test_labels = [line.strip().split(\"\\t\") for line in f.readlines()]  # Adapter selon le format du fichier\n",
    "\n",
    "# Convertir les labels en DataFrame\n",
    "df_test_labels = pd.DataFrame(test_labels, columns=[\"id\", \"Parti Politique\"])\n",
    "\n",
    "merge_data_test =  pd.concat([data_test, df_test_labels], axis=1)\n",
    "merge_data_test = merge_data_test[[\"Texte\",\"Parti Politique\"]]\n",
    "merge_data_test = merge_data_test.dropna()\n",
    "\n",
    "X_data_test = merge_data_test['Texte']\n",
    "y_data_test = merge_data_test['Parti Politique']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parti Politique\n",
      "ELDR         2008\n",
      "GUE-NGL      2008\n",
      "PPE-DE       2008\n",
      "PSE          2008\n",
      "Verts-ALE    2008\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "\n",
    "# Trouver la taille minimale\n",
    "min_size = train_df['Parti Politique'].value_counts().min()\n",
    "\n",
    "# Grouper par classe et sous-échantillonner\n",
    "balanced_train = pd.concat([\n",
    "    resample(group, replace=False, n_samples=min_size, random_state=42)\n",
    "    for _, group in train_df.groupby('Parti Politique')\n",
    "])\n",
    "\n",
    "print(balanced_train['Parti Politique'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parti Politique\n",
      "ELDR         1339\n",
      "GUE-NGL      1339\n",
      "PPE-DE       1339\n",
      "PSE          1339\n",
      "Verts-ALE    1339\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "\n",
    "# Trouver la taille minimale\n",
    "min_size = merge_data_test['Parti Politique'].value_counts().min()\n",
    "\n",
    "# Grouper par classe et sous-échantillonner\n",
    "balanced_test = pd.concat([\n",
    "    resample(group, replace=False, n_samples=min_size, random_state=42)\n",
    "    for _, group in train_df.groupby('Parti Politique')\n",
    "])\n",
    "print(balanced_test['Parti Politique'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_data_train = balanced_train['Texte']\n",
    "# y_data_train = balanced_train['Parti Politique']\n",
    "\n",
    "# X_data_test = balanced_test['Texte']\n",
    "# y_data_test = balanced_test['Parti Politique'] \n",
    "# X_data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19370"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7279907084785133\n",
      "\n",
      "Rapport de classification :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        ELDR       0.78      0.67      0.72      1339\n",
      "     GUE-NGL       0.70      0.80      0.75      1793\n",
      "      PPE-DE       0.75      0.74      0.74      4571\n",
      "         PSE       0.69      0.72      0.71      3627\n",
      "   Verts-ALE       0.73      0.68      0.71      1585\n",
      "\n",
      "    accuracy                           0.73     12915\n",
      "   macro avg       0.73      0.72      0.73     12915\n",
      "weighted avg       0.73      0.73      0.73     12915\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "\n",
    "# Vectorisation TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_data_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_data_test)\n",
    "\n",
    "# Entraîner un modèle SVM\n",
    "svm_classifier = SVC(kernel='rbf', class_weight='balanced', random_state=42)\n",
    "\n",
    "svm_classifier.fit(X_train_tfidf, y_data_train)\n",
    "\n",
    "# Prédictions sur le jeu de test\n",
    "y_pred = svm_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Évaluation des performances\n",
    "accuracy = accuracy_score(y_data_test, y_pred)\n",
    "print(\"Accuracy :\", accuracy)\n",
    "print(\"\\nRapport de classification :\\n\", classification_report(y_data_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Représentation par espaces de thèmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_stop_words = stopwords.words('french')  # Vérifiez l'importation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thème 0:\n",
      "président monsieur cette question contre fait présidente si plus être\n",
      "Thème 1:\n",
      "pays union européenne président monsieur politique plus europe droits états\n",
      "Thème 2:\n",
      "commission président monsieur parlement rapport voudrais cette conseil tout madame\n",
      "Thème 3:\n",
      "plus être européenne monsieur états cette président marché membres comme\n",
      "Thème 4:\n",
      "monsieur président commissaire rapport messieurs mesdames plus madame collègues fait\n",
      "Accuracy: 0.24359272164150214\n",
      "Rapport de classification:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ELDR       0.11      0.14      0.12      1339\n",
      "     GUE-NGL       0.25      0.57      0.35      1793\n",
      "      PPE-DE       0.42      0.06      0.10      4571\n",
      "         PSE       0.34      0.35      0.35      3627\n",
      "   Verts-ALE       0.14      0.24      0.17      1585\n",
      "\n",
      "    accuracy                           0.24     12915\n",
      "   macro avg       0.25      0.27      0.22     12915\n",
      "weighted avg       0.31      0.24      0.22     12915\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Vectorisation des textes\n",
    "vectorizer = CountVectorizer(\n",
    "    max_df=0.95, min_df=2, stop_words=french_stop_words\n",
    ")\n",
    "X_train_counts = vectorizer.fit_transform(X_data_train)\n",
    "X_test_counts = vectorizer.transform(X_data_test)\n",
    "\n",
    "# Entraîner LDA\n",
    "lda_model = LatentDirichletAllocation(\n",
    "    n_components=5,  # Nombre de thèmes\n",
    "    doc_topic_prior=1.0,  # Prior des thèmes dans chaque document\n",
    "    topic_word_prior=0.8,  # Prior des mots dans chaque thème\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Apprentissage sur les données d'entraînement\n",
    "X_train_lda = lda_model.fit_transform(X_train_counts)\n",
    "\n",
    "# Projection des données de test dans l'espace des thèmes\n",
    "X_test_lda = lda_model.transform(X_test_counts)\n",
    "\n",
    "# Afficher les thèmes\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    print(f\"Thème {topic_idx}:\")\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-11:-1]]))\n",
    "\n",
    "# Entraîner un classifieur SVM avec les représentations thématiques\n",
    "svm_classifier = SVC(kernel='rbf', class_weight='balanced', random_state=42)\n",
    "svm_classifier.fit(X_train_lda, y_data_train)\n",
    "\n",
    "# Prédictions et évaluation\n",
    "y_pred = svm_classifier.predict(X_test_lda)\n",
    "accuracy = accuracy_score(y_data_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Rapport de classification:\")\n",
    "print(classification_report(y_data_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
